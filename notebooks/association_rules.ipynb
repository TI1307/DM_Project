{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Association Rules\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, apriori, association_rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "Borrowings_Table = pd.read_excel('../data/Clean_Data/cleaned_borrowings.xlsx')\n",
        "\n",
        "# Group by user to create transactions\n",
        "Borrowings_transactions = Borrowings_Table.groupby('N° lecteur')['Titre_clean'].apply(list).reset_index()\n",
        "\n",
        "# Create transaction list\n",
        "transactions = Borrowings_transactions['Titre_clean'].tolist()\n",
        "\n",
        "# Transform to binary matrix\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "borrowing_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "print(f'Total transactions: {len(borrowing_df)}')\n",
        "print(f'Unique books: {len(borrowing_df.columns)}')\n",
        "print(f'Average books per transaction: {borrowing_df.sum(axis=1).mean():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Categorize user by Borrowings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "Borrowings_transactions['num_books_borrowed'] = Borrowings_transactions['Titre_clean'].apply(len)\n",
        "# Create borrowing categories\n",
        "borrowing_categories = pd.cut(Borrowings_transactions['num_books_borrowed'],\n",
        "                              bins=[0, 1, 2, 3, 5, float('inf')],\n",
        "                              labels=['1 book', '2 books', '3 books', '4-5 books', '6+ books'])\n",
        "\n",
        "category_counts = borrowing_categories.value_counts().sort_index()\n",
        "\n",
        "# Create pie chart\n",
        "colors = plt.cm.Set3(range(len(category_counts)))\n",
        "wedges, texts, autotexts = plt.pie(category_counts, \n",
        "                                     labels=category_counts.index,\n",
        "                                     autopct='%1.1f%%',\n",
        "                                     startangle=90,\n",
        "                                     colors=colors,\n",
        "                                     textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
        "\n",
        "# Add counts to labels\n",
        "for i, (label, count) in enumerate(zip(category_counts.index, category_counts.values)):\n",
        "    texts[i].set_text(f'{label}\\n({count} users)')\n",
        "\n",
        "plt.title('User Segmentation by Borrowing Frequency', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('user_segmentation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Support Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate support for all items\n",
        "item_support = borrowing_df.sum() / len(borrowing_df)\n",
        "item_support_df = pd.DataFrame({\n",
        "    'item': borrowing_df.columns,\n",
        "    'support': item_support.values\n",
        "}).sort_values('support', ascending=False)\n",
        "\n",
        "# Statistics\n",
        "print('Support Statistics:')\n",
        "print(f'  Mean: {item_support.mean():.4f}')\n",
        "print(f'  Median: {item_support.median():.4f}')\n",
        "print(f'  Std Dev: {item_support.std():.4f}')\n",
        "print(f'  Min: {item_support.min():.4f}')\n",
        "print(f'  Max: {item_support.max():.4f}')\n",
        "print(f'\\nTop 10 most frequent items:')\n",
        "print(item_support_df.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Threshold Sensitivity Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test various support thresholds\n",
        "support_values = [0.003, 0.0037, 0.005, 0.0075, 0.01, 0.015, 0.02, 0.025, 0.03]\n",
        "support_analysis = []\n",
        "\n",
        "print('Testing support thresholds...')\n",
        "for min_sup in support_values:\n",
        "    freq_itemsets = fpgrowth(borrowing_df, min_support=min_sup, use_colnames=True)\n",
        "    \n",
        "    singles = freq_itemsets[freq_itemsets['itemsets'].apply(lambda x: len(x) == 1)]\n",
        "    pairs = freq_itemsets[freq_itemsets['itemsets'].apply(lambda x: len(x) == 2)]\n",
        "    triples = freq_itemsets[freq_itemsets['itemsets'].apply(lambda x: len(x) >= 3)]\n",
        "    \n",
        "    support_analysis.append({\n",
        "        'min_support': min_sup,\n",
        "        'total_itemsets': len(freq_itemsets),\n",
        "        'single_items': len(singles),\n",
        "        'pairs': len(pairs),\n",
        "        'triples_plus': len(triples),\n",
        "        'max_size': freq_itemsets['itemsets'].apply(len).max()\n",
        "    })\n",
        "\n",
        "support_df = pd.DataFrame(support_analysis)\n",
        "print('\\nSupport Analysis Results:')\n",
        "print(support_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select Optimal Support\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the prev analysis , we fount that **min_sup=0.003** gives a **lot** of of total itemsets , but for **min_sup=0.0037/0.005** its **consistent** and managable , and the other values gives a **few** total itemsets . So we choose **0.005** as min sup ( when u doubt go higher )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select optimal support\n",
        "\n",
        "optimal_support = 0.005\n",
        "\n",
        "print(f'Selected optimal support: {optimal_support}')\n",
        "# Generate frequent itemsets\n",
        "frequent_itemsets = fpgrowth(borrowing_df, min_support=optimal_support, use_colnames=True)\n",
        "print(f'\\n✓ Generated {len(frequent_itemsets)} frequent itemsets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence Threshold Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test various confidence thresholds\n",
        "confidence_values = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
        "confidence_analysis = []\n",
        "\n",
        "print('Testing confidence thresholds...')\n",
        "for min_conf in confidence_values:\n",
        "    try:\n",
        "        rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_conf)\n",
        "        \n",
        "        if len(rules) > 0:\n",
        "            confidence_analysis.append({\n",
        "                'min_confidence': min_conf,\n",
        "                'num_rules': len(rules),\n",
        "                'avg_confidence': rules['confidence'].mean(),\n",
        "                'avg_lift': rules['lift'].mean(),\n",
        "                'strong_rules': len(rules[rules['lift'] > 1.2])\n",
        "            })\n",
        "        else:\n",
        "            confidence_analysis.append({\n",
        "                'min_confidence': min_conf,\n",
        "                'num_rules': 0,\n",
        "                'avg_confidence': 0,\n",
        "                'avg_lift': 0,\n",
        "                'strong_rules': 0\n",
        "            })\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "confidence_df = pd.DataFrame(confidence_analysis)\n",
        "print('\\nConfidence Analysis Results:')\n",
        "print(confidence_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Final Rules with Optimal Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the analysis , we choose the min_confdence because it **Balance between Quality and Quantity**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select optimal confidence\n",
        "optimal_confidence = 0.3\n",
        "\n",
        "print(f'OPTIMAL PARAMETERS SELECTED:')\n",
        "print(f'  Minimum Support: {optimal_support}')\n",
        "print(f'  Minimum Confidence: {optimal_confidence}')\n",
        "\n",
        "# Generate rules\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=optimal_confidence)\n",
        "rules = rules.sort_values('lift', ascending=False)\n",
        "\n",
        "print(f'\\nGenerated {len(rules)} association rules')\n",
        "print(f'  Average confidence: {rules[\"confidence\"].mean():.3f}')\n",
        "print(f'  Average lift: {rules[\"lift\"].mean():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display top rules\n",
        "print('\\n' + '='*120)\n",
        "print('TOP 10 ASSOCIATION RULES (by Lift)')\n",
        "print('='*120)\n",
        "\n",
        "for idx, (i, rule) in enumerate(rules.head(10).iterrows(), 1):\n",
        "    antecedent = ', '.join(list(rule['antecedents']))[:60]\n",
        "    consequent = ', '.join(list(rule['consequents']))[:60]\n",
        "    \n",
        "    print(f'\\nRule {idx}:')\n",
        "    print(f'  IF user borrows: {antecedent}')\n",
        "    print(f'  THEN likely to borrow: {consequent}')\n",
        "    print(f'  Confidence: {rule[\"confidence\"]:.3f} | Lift: {rule[\"lift\"]:.3f} | Support: {rule[\"support\"]:.4f}')\n",
        "    print('-'*120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "here we noticed that the support of all the rules equals to 0.0074 , so we decided to analyse the support of all the itemsets that contains more then 1 item ( which we are gonna construct the rules from )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter only itemsets with size >= 2\n",
        "double_plus = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) >= 2)]\n",
        "\n",
        "# Compute support statistics\n",
        "support_values = double_plus['support']\n",
        "\n",
        "print(\"Support Statistics for itemsets with size >= 2:\")\n",
        "print(f\"  Count: {len(support_values)}\")\n",
        "print(f\"  Mean: {support_values.mean():.4f}\")\n",
        "print(f\"  Median: {support_values.median():.4f}\")\n",
        "print(f\"  Standard Deviation: {support_values.std():.4f}\")\n",
        "print(f\"  Minimum Support: {support_values.min():.4f}\")\n",
        "print(f\"  Maximum Support: {support_values.max():.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 double+ by support:\")\n",
        "print(double_plus.sort_values('support', ascending=False).head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So why those item sets with hight support , didnt apear in the best rules ??\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assuming 'rules' is your association rules DataFrame\n",
        "plt.figure(figsize=(10, 7))\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.0)\n",
        "\n",
        "# scatter plot of support vs confidence, colored by lift\n",
        "scatter = plt.scatter(\n",
        "    rules['support'],\n",
        "    rules['confidence'],\n",
        "    c=rules['lift'],\n",
        "    cmap='viridis',\n",
        "    alpha=0.7\n",
        ")\n",
        "# Threshold lines\n",
        "plt.axvline(x=0.005, color='red', linestyle='--', label='Min Support = 0.005')\n",
        "plt.axhline(y=0.3, color='blue', linestyle='--', label='Min Confidence = 0.3')\n",
        "plt.colorbar(scatter, label='Lift')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Confidence')\n",
        "plt.title('Association Rules: Support vs Confidence (colored by Lift)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export rules\n",
        "rules_export = rules.copy()\n",
        "rules_export['antecedents'] = rules_export['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
        "rules_export['consequents'] = rules_export['consequents'].apply(lambda x: ', '.join(list(x)))\n",
        "rules_export.to_csv('../data/association_result/association_rules_final.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*80)\n",
        "print('ASSOCIATION RULES ANALYSIS - FINAL SUMMARY')\n",
        "print('='*80)\n",
        "print(f'\\nOPTIMAL PARAMETERS:')\n",
        "print(f'  Minimum Support: {optimal_support}')\n",
        "print(f'  Minimum Confidence: {optimal_confidence}')\n",
        "print(f'\\nRESULTS:')\n",
        "print(f'  Frequent Itemsets: {len(frequent_itemsets)}')\n",
        "print(f'  Association Rules: {len(rules)}')\n",
        "print(f'  Average Confidence: {rules[\"confidence\"].mean():.3f}')\n",
        "print(f'  Average Lift: {rules[\"lift\"].mean():.3f}')\n",
        "print(f'  Strong Rules (Lift>1.5): {len(rules[rules[\"lift\"]>1.5])} ({len(rules[rules[\"lift\"]>1.5])/len(rules)*100:.1f}%)')\n",
        "print(f'\\nJUSTIFICATION:')\n",
        "print(f'  - Support threshold selected through sensitivity analysis')\n",
        "print(f'  - Confidence threshold optimized for rule quality (lift)')\n",
        "print(f'  - {len(rules[rules[\"lift\"]>1.2])/len(rules)*100:.0f}% of rules show genuine positive associations')\n",
        "print(f'  - Parameters validated with multiple quality metrics')\n",
        "print('='*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DM_ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
